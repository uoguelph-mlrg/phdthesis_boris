\chapter{Concluding Remarks}

This thesis explores generalization in graph reasoning tasks such as graph classification, compositional visual reasoning using scene graphs and reasoning about neural networks (NNs).
Approaching these tasks using models that learn from data instead of engineering features is a de facto standard. Nevertheless, one of the fundamental challenges of such models, in particular NNs, is poor generalization. This issue is perhaps due to models' reliance on spurious correlations (``shortcuts'') that are often abundant in training data~\citep{shen2021towards,zhou2021domain,scholkopf2021toward}. The issue is particularly noticeable when NNs are evaluated on test data that are sampled from a slightly different distribution as compared to the training data. 
While humans often show strong generalization to various distribution shifts, machine learning models are much weaker in that respect. 
This thesis makes several contributions towards understanding and improving generalization.\looseness-1 

In Chapter~\ref{ch:neurips2019}, we address generalization in classic graph tasks. Machine learning on graphs and developing stronger and more expressive graph neural networks (GNNs) to solve them have become increasingly popular topics in artificial intelligence.
Generalization of GNNs to different distribution shifts remains a fundamental issue. Some distribution shifts, such as increased graph size at test time, are studied more in detail than others.
In particular, \cite{yehudai2021local} show the existence of a fundamental issue in GNNs as they converge to a ``bad'' optimum under certain assumptions preventing generalization to larger graphs. Along with our work, additional supervision improved their generalization results. \cite{bevilacqua2021size} address the size generalization problem by explicitly modeling a size-invariant representation.
In practice, different distribution shifts might arise, such as noisy node features in our work. Modeling each distribution shift explicitly is a time consuming process. Instead, it is desirable to learn to generalize to diverse distribution shifts. However, this is perhaps an ill-posed task. Therefore, a possible direction to improve generalization without explicitly hardcoding it is to follow the meta-learning idea of ``learning to generalize''~\citep{finn2017model}. Another direction is to learn more generic models that have access to more diverse knowledge similar to humans~\citep{brown2020language,cobbe2021training}. These models can be fine-tuned to diverse tasks and potentially can have better generalization by having a strong inductive prior. In general, the approaches of improving generalization of other kinds of NNs (especially CNNs) are possible to be applied to GNNs and vice versa due the inherent similarity between different kinds of NNs (\S~\ref{sec:bg_nn}). Therefore, transferring generalization approaches from one domain to another should be studied more.\looseness-1


In Chapters~\ref{ch:bmvc2020} and~\ref{ch:iccv2021}, we address the compositional generalization problem in visual reasoning, specifically in the scene graph generation (SGG) task. SGG models typically have two stages: (i) object detection and (ii) compositional reasoning that predicts scene graphs based on features from the detector. Our works only improve the second stage. Meanwhile, it is possible that the compositional generalization issue primarily arises from the first stage due to the ``bad'' bias learned by the detectors~\citep{michaelis2019benchmarking}, which is hard to fix in the second stage.
Therefore, one approach to further improve generalization in SGG tasks is to improve generalization of object detectors~\citep{shen2021towards}. Object detectors are in turn built on image classification networks, so improving generalization in image classification can indirectly lead to improved compositional generalization in SGG~\citep{li2020shape,chen2021robust}. Another approach to improve compositional generalization in SGG can be based on learning a more generic language~\citep{brown2020language,cobbe2021training} or multimodal~\citep{radford2021learning} model along the lines of the discussion in the previous paragraph. In particular, the SGG task is closely related to natural language processing, so by leveraging a strong prior from language models better compositional generalization in vision can be achieved.\looseness-1

In Chapter~\ref{ch:neurips2021}, we explore a novel graph reasoning task, in which models must generalize to novel graphs in a non-trivial way. In particular, we introduced a parameter prediction task, in which a single GNN (more specifically, GHN) must predict parameters for unseen deep neural networks represented as graphs. Generalization to unseen networks is extremely challenging even if the networks are \IID sampled (see our work~\citep{knyazev2021parameter} as well as works on meta-optimizers~\citep{wichrowska2017learned,metz2020tasks}). The challenge is due to a complex interplay between thousands or millions of neurons in a NN. To study generalization of GHNs more systematically, we evaluate generalization to specific distribution shifts, including NN's capacity, graph size and complexity of connections.
Despite surprisingly good results of our GHN in certain cases, scaling our GHN to larger datasets is an important future direction to increase the practical value of our GHN. Another important direction to pursue is to enable generalization of GHNs across tasks along with meta-optimizers~\citep{metz2020tasks} and other hypernetwork works~\citep{anonymous2021HyperTransformer}. This way, GHNs has a potential to become more practical for end users to solve their tasks with little computational resources available. Thus, our work makes an important step towards democratization of machine learning~\citep{ahmed2020democratization,abdalla2021grey}.
